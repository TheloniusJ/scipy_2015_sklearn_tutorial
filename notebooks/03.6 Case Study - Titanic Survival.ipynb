{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will talk about an important piece of machine learning: the extraction of\n",
    "quantitative features from data.  By the end of this section you will\n",
    "\n",
    "- Know how features are extracted from real-world data.\n",
    "- See an example of extracting numerical features from textual data\n",
    "\n",
    "In addition, we will go over several basic tools within scikit-learn which can be used to accomplish the above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that data in scikit-learn is expected to be in two-dimensional arrays, of size\n",
    "**n_samples** $\\times$ **n_features**.\n",
    "\n",
    "Previously, we looked at the iris dataset, which has 150 samples and 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features are:\n",
    "\n",
    "- sepal length in cm\n",
    "- sepal width in cm\n",
    "- petal length in cm\n",
    "- petal width in cm\n",
    "\n",
    "Numerical features such as these are pretty straightforward: each sample contains a list\n",
    "of floating-point numbers corresponding to the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you have categorical features?  For example, imagine there is data on the color of each\n",
    "iris:\n",
    "\n",
    "    color in [red, blue, purple]\n",
    "\n",
    "You might be tempted to assign numbers to these features, i.e. *red=1, blue=2, purple=3*\n",
    "but in general **this is a bad idea**.  Estimators tend to operate under the assumption that\n",
    "numerical features lie on some continuous scale, so, for example, 1 and 2 are more alike\n",
    "than 1 and 3, and this is often not the case for categorical features.\n",
    "\n",
    "A better strategy is to give each category its own dimension.  \n",
    "The enriched iris feature set would hence be in this case:\n",
    "\n",
    "- sepal length in cm\n",
    "- sepal width in cm\n",
    "- petal length in cm\n",
    "- petal width in cm\n",
    "- color=purple (1.0 or 0.0)\n",
    "- color=blue (1.0 or 0.0)\n",
    "- color=red (1.0 or 0.0)\n",
    "\n",
    "Note that using many of these categorical features may result in data which is better\n",
    "represented as a **sparse matrix**, as we'll see with the text classification example\n",
    "below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the DictVectorizer to encode categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the source data is encoded has a list of dicts where the values are either strings names for categories or numerical values, you can use the `DictVectorizer` class to compute the boolean expansion of the categorical features while leaving the numerical features unimpacted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Fransisco', 'temperature': 18.},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec.fit_transform(measurements).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common feature type are **derived features**, where some pre-processing step is\n",
    "applied to the data to generate features that are somehow more informative.  Derived\n",
    "features may be based in **dimensionality reduction** (such as PCA or manifold learning),\n",
    "may be linear or nonlinear combinations of features (such as in Polynomial regression),\n",
    "or may be some more sophisticated transform of the features.  The latter is often used\n",
    "in image processing.\n",
    "\n",
    "For example, [scikit-image](http://scikit-image.org/) provides a variety of feature\n",
    "extractors designed for image data: see the ``skimage.feature`` submodule.\n",
    "We will see some *dimensionality*-based feature extraction routines later in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of how to work with both categorical and numerical data, we will perform survival predicition for the passengers of the HMS Titanic.\n",
    "\n",
    "We will use a version of the Titanic (titanic3.xls) dataset from Thomas Cason, as retrieved from Frank Harrell's webpage [here](http://lib.stat.cmu.edu/S/Harrell/data/descriptions/titanic.html). We converted the .xls to .csv for easier manipulation without involving external libraries, but the data is otherwise unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "f = open(os.path.join('datasets', 'titanic', 'titanic3.csv'))\n",
    "# Remove . from home.dest, split on quotes because some fields have commas\n",
    "keys = f.readline().strip().replace('.', '').split('\",\"')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in all the lines from the (titanic3.csv) file, and set aside the keys from the first line. Let's look at the keys and some corresponding example lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(keys)\n",
    "print()\n",
    "for i in range(3):\n",
    "    print(lines[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The site [linked here](http://lib.stat.cmu.edu/S/Harrell/data/descriptions/titanic3info.txt) gives a broad description of the keys and what they mean - we show it here for completeness\n",
    "\n",
    "```\n",
    "pclass          Passenger Class\n",
    "                (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "survival        Survival\n",
    "                (0 = No; 1 = Yes)\n",
    "name            Name\n",
    "sex             Sex\n",
    "age             Age\n",
    "sibsp           Number of Siblings/Spouses Aboard\n",
    "parch           Number of Parents/Children Aboard\n",
    "ticket          Ticket Number\n",
    "fare            Passenger Fare\n",
    "cabin           Cabin\n",
    "embarked        Port of Embarkation\n",
    "                (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "boat            Lifeboat\n",
    "body            Body Identification Number\n",
    "home.dest       Home/Destination\n",
    "```\n",
    "\n",
    "In general, it looks like `name`, `sex`, `cabin`, `embarked`, `boat`, `body`, and `homedest` may be candidates for categorical features, while the rest appear to be numerical features. We can now write a function to extract features from a text line, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can also use pandas\n",
    "def process_line(line):\n",
    "    # Split line on \",\" to get fields without comma confusion\n",
    "    vals = line.strip().split('\",')\n",
    "    # replace spurious \" characters \n",
    "    vals = [v.replace('\"', '') for v in vals]\n",
    "    pclass = int(vals[0])\n",
    "    survived = int(vals[1])\n",
    "    name = str(vals[2])\n",
    "    sex = str(vals[3])\n",
    "    try:\n",
    "        age = float(vals[4])\n",
    "    except ValueError:\n",
    "        # Blank age\n",
    "        age = -1\n",
    "    sibsp = float(vals[5])\n",
    "    parch = int(vals[6])\n",
    "    ticket = str(vals[7])\n",
    "    try:\n",
    "        fare = float(vals[8])\n",
    "    except ValueError:\n",
    "        # Blank fare\n",
    "        fare = -1\n",
    "    cabin = str(vals[9])\n",
    "    embarked = str(vals[10])\n",
    "    boat = str(vals[11])\n",
    "    homedest = str(vals[12])\n",
    "    line_dict = {'pclass': pclass, 'survived': survived, 'name': name, 'sex': sex, 'age': age, 'sibsp': sibsp,\n",
    "                 'parch': parch, 'ticket': ticket, 'fare': fare, 'cabin': cabin, 'embarked': embarked,\n",
    "                 'boat': boat, 'homedest': homedest}\n",
    "    return line_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process an example line using this function to see the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(process_line(lines[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we seem to have extracted relevant features, and found a representation that will eventually work with ``DictVectorizer``. However, first we need to break the dataset into six pieces: training categorical, training numeric, testing categorical, testing numeric, training labels, and testing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_keys = ['name', 'sex', 'ticket', 'cabin', 'embarked', 'boat', 'homedest']\n",
    "numeric_keys = ['pclass', 'survived', 'age', 'sibsp', 'parch', 'fare']\n",
    "train_vectorizer_list = []\n",
    "test_vectorizer_list = []\n",
    "\n",
    "n_samples = len(lines)\n",
    "numeric_data = np.zeros((n_samples, len(numeric_keys) - 1))\n",
    "numeric_labels = np.zeros((n_samples,), dtype=int)\n",
    "\n",
    "random_state = np.random.RandomState(1999)\n",
    "indices = np.arange(len(lines))\n",
    "random_state.shuffle(indices)\n",
    "# 1309 total samples - use random 1100 to train and 209 to test\n",
    "train_idx = indices[:1100]\n",
    "test_idx = indices[1100:]\n",
    "\n",
    "for n, l in enumerate(lines):\n",
    "    line_dict = process_line(l)\n",
    "    strings = {k: line_dict[k] for k in string_keys}\n",
    "    if n in train_idx:\n",
    "        train_vectorizer_list.append(strings)\n",
    "    else:\n",
    "        test_vectorizer_list.append(strings)\n",
    "    numeric_data[n] = np.asarray([line_dict[k] for k in numeric_keys if k != \"survived\"])\n",
    "    numeric_labels[n] = line_dict[\"survived\"]\n",
    "\n",
    "train_numeric = numeric_data[train_idx]\n",
    "test_numeric = numeric_data[test_idx]\n",
    "train_labels = numeric_labels[train_idx]\n",
    "test_labels = numeric_labels[test_idx]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "# .toarray() due to returning a scipy sparse array\n",
    "train_categorical = vec.fit_transform(train_vectorizer_list).toarray()\n",
    "test_categorical = vec.transform(test_vectorizer_list).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine everything into training and testing datasets by combining the categorical and numeric data, concatenating along the feature axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.concatenate([train_numeric, train_categorical], axis=1)\n",
    "test_data = np.concatenate([test_numeric, test_categorical], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of the hard data work out of the way, evaluating a classifier on this data becomes straightforward. Using a RandomForestClassifier we see decent performance - about 70% accuracy in predicting who survives the Titanic disaster. With random chance being 50%, we are doing a decent job, though there are many refinements which could be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(train_data, train_labels)\n",
    "pred_labels = clf.predict(test_data)\n",
    "print(\"Prediction accuracy: %f\" % accuracy_score(pred_labels, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exercise\n",
    "=====\n",
    "\n",
    "Can you remove or create new features to improve your score? Try plotting feature importance as shown in this [sklearn example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) or removing features like ``name``, ``body``, and ``homedest``. Can you fill in missing data (see the try, except pieces of process_line) and improve your score further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
